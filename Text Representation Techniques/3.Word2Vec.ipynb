{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Word Embedding: Word2Vec\n",
    "Word2Vec is a popular Word Embedding technique that represents words as dense vectors, positioning them in a multi-dimensional space. This allows words with similar meanings to be closer in the vector space, enabling the model to capture semantic relationships.\n",
    "\n",
    "Word2Vec operates on the principle that words appearing in similar contexts share semantic meanings. It learns vector representations by considering the words surrounding a target word in a given context. There are two main architectures for Word2Vec: Continuous Bag of Words (CBOW) and Skip-Gram. CBOW predicts a target word based on its context, while Skip-Gram predicts the context words given a target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = [\"I am Aftab Mallick\",\n",
    "               \"I am Interested in learning NLP\",\n",
    "               \"I know Machine Learning\"]\n",
    "tokenize_documents = [word_tokenize(word.lower()) for word in sample_text]\n",
    "\n",
    "model = Word2Vec(sentences = tokenize_documents,vector_size=50,workers=8,window=10,min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.8740549e-03, -5.2920175e-03, -1.4147566e-02, -1.5610614e-02,\n",
       "       -1.8243574e-02, -1.1870339e-02, -3.6948491e-03, -8.6477427e-03,\n",
       "       -1.2921341e-02, -7.4346447e-03,  8.5783172e-03, -7.4780867e-03,\n",
       "        1.6756350e-02,  3.0679870e-03, -1.4484639e-02,  1.8867597e-02,\n",
       "        1.5262425e-02,  1.0986564e-02, -1.3697691e-02,  1.1645358e-02,\n",
       "        8.0181863e-03,  1.0370739e-02,  8.5118031e-03,  3.8795089e-03,\n",
       "       -6.3403249e-03,  1.6707690e-02,  1.9224361e-02,  7.5852061e-03,\n",
       "       -5.6739901e-03,  1.4255047e-05,  2.4376370e-03, -1.6916649e-02,\n",
       "       -1.6447891e-02, -4.6203137e-04,  2.4745751e-03, -1.1486761e-02,\n",
       "       -9.4505474e-03, -1.4692149e-02,  1.6657231e-02,  2.4259568e-04,\n",
       "       -9.0187974e-03,  1.1403411e-02,  1.8360030e-02, -8.1997439e-03,\n",
       "        1.5929364e-02,  1.0750868e-02,  1.1758246e-02,  1.0251808e-03,\n",
       "        1.6426168e-02, -1.4038081e-02], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['nlp']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
