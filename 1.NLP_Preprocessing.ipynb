{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization and Text Cleaning\n",
    "At the heart of NLP lies the art of breaking down text into meaningful units. Tokenization is the process of splitting text into words, phrases, or even sentences (tokens). It’s the initial step that sets the stage for further analysis. Coupled with text cleaning, where we remove unnecessary characters, numbers, and symbols, tokenization ensures we work with pristine, understandable language units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'is', 'a', 'student', '!', 'He', 'loves', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "text = \"Aftab is a student! He loves Natural Language Processing.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftab', 'is', 'a', 'student', 'he', 'loves', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Now we will clean it\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stop Words Removing:\n",
    "Not all words contribute equally to the meaning of a sentence. Stop words like “the” or “and” are often filtered out to focus on more meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'student', '!', 'He', 'loves', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "filtered_tokens =[token for token in tokens if token not in stopwords]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
