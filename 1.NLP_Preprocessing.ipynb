{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization and Text Cleaning\n",
    "At the heart of NLP lies the art of breaking down text into meaningful units. Tokenization is the process of splitting text into words, phrases, or even sentences (tokens). Itâ€™s the initial step that sets the stage for further analysis. Coupled with text cleaning, where we remove unnecessary characters, numbers, and symbols, tokenization ensures we work with pristine, understandable language units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'is', 'a', 'student', '!', 'He', 'loves', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "text = \"Aftab is a student! He loves Natural Language Processing.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftab', 'is', 'a', 'student', 'he', 'loves', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Now we will clean it\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
