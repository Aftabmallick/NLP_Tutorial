{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization and Text Cleaning\n",
    "At the heart of NLP lies the art of breaking down text into meaningful units. Tokenization is the process of splitting text into words, phrases, or even sentences (tokens). It’s the initial step that sets the stage for further analysis. Coupled with text cleaning, where we remove unnecessary characters, numbers, and symbols, tokenization ensures we work with pristine, understandable language units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'is', 'a', 'student', '!', 'He', 'loves', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "text = \"Aftab is a student! He loves Natural Language Processing.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftab', 'is', 'a', 'student', 'he', 'loves', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Now we will clean it\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stop Words Removing:\n",
    "Not all words contribute equally to the meaning of a sentence. Stop words like “the” or “and” are often filtered out to focus on more meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'student', '!', 'He', 'loves', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "filtered_tokens =[token for token in tokens if token not in stopwords]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming and Lemmatizing\n",
    "Stemming and lemmatization are both text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root forms. While they share the goal of simplifying words, they operate differently in terms of the linguistic knowledge they apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a. Stemming: Reducing to Root Forms\n",
    "\n",
    "Stemming involves cutting off prefixes or suffixes of words to obtain their root or base form, known as the stem. The purpose is to treat words with similar meanings as if they were the same. Stemming is a rule-based method that doesn’t always result in a valid word, but it’s computationally less intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftab', 'student', '!', 'he', 'love', 'natur', 'languag', 'process', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b. Lemmatization: Transforming to Dictionary Form\n",
    "\n",
    "Lemmatization, on the other hand, involves reducing words to their base or dictionary forms, known as lemmas. It takes into account the context of the word in a sentence and applies morphological analysis. Lemmatization results in valid words and is more linguistically informed compared to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aftab', 'student', '!', 'He', 'love', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Part-of-Speech Tagging:\n",
    "Part-of-speech tagging (POS tagging) is a natural language processing task where the goal is to assign a grammatical category (such as noun, verb, adjective, etc.) to each word in a given text. This provides a deeper understanding of the structure and function of each word in a sentence.\n",
    "<a href= \"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">The Penn Treebank POS Tag Set</a> is a widely used standard for representing these part-of-speech tags in English text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aftab', 'NNP'), ('student', 'NN'), ('!', '.'), ('He', 'PRP'), ('loves', 'VBZ'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "#nltk.download()   #Uncomment if getting error\n",
    "pos_tags=nltk.pos_tag(filtered_tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Named Entity Recognition (NER):\n",
    "NER takes language understanding to the next level by identifying and classifying entities like names, locations, organizations, etc., in a given text. This is crucial for extracting meaningful information from unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Aftab/NNP)\n",
      "  student/NN\n",
      "  !/.\n",
      "  He/PRP\n",
      "  loves/VBZ\n",
      "  (ORGANIZATION Natural/JJ Language/NNP)\n",
      "  Processing/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
